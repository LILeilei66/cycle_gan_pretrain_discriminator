from .base_model import BaseModel
from . import networks
from torch import optim
import itertools

class DiscriminateModel(BaseModel):
    """
    This DescriminateModel can be used to classify inputs into two categories.
    attributions:
        netD:           本类不包含Generators.
        criterion:      使用LossGAN, 根本上来说是MSELoss.
        loss_names：     'D', 对于discriminator training 只需要loss_D.
        visual_names:
        model_names:
    """

    @staticmethod
    def modify_commandline_options(parser, is_train=True):
        assert not is_train, 'DescriminateModel cannot be used during training time'
        parser.set_defaults(dataset_mode='single')
        parser.add_argument('--model_suffix', type=str, default='',
                            help='In checkpoints_dir, [epoch]_net_G[model_suffix].pth will be loaded as the generator.')
        return parser

    def __init__(self, opt):
        """Initialize the DescriminateModel class.

        Parameters:
            opt:        存储所有需要的参量, 可由外部构建, 无需继承BaseOptions()
        """
        assert(opt.isTrain) # 若 isTrain, 则必须loadNet
        BaseModel.__init__(self, opt)  # specify the training losses you want to print out. The

        # training/test scripts will call <BaseModel.get_current_losses>
        self.loss_names = ['D'] # 只需要 loss_D
        # specify the images you want to save/display. The training/test scripts  will call
        # <BaseModel.get_current_visuals>
        self.visual_names = []
        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>
        self.model_names = ['D' + opt.model_suffix] # only discriminators
        # are needed.
        self.netD = networks.define_D(opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D, opt.norm,
                                      opt.init_type, opt.init_gain)

        # assigns the model to self.netD_[suffix] so that it can be loaded
        # please see <BaseModel.load_networks>
        setattr(self, 'netD' + opt.model_suffix, self.netD)  # store netG in self.

        self.optimizer_D = optim.Adam()
        self.criterion = networks.GANLoss(opt.gan_mode).to(self.device)

    def set_input(self, input):
        """
        Parameters:
            input:      {'image': ndarray type, 'label': 0 或 1}.
        """
        self.real_A = input['image'].to(self.device)
        print(self.real_A.shape)

    def forward(self):
        """Run forward pass."""
        self.features = self.netD(self.real_A) # D(A)

    def backward_D_basic(self, netD, real, fake):
        """Calculate GAN loss for the discriminator

        Parameters:
            netD (network)      -- the discriminator D
            real (tensor array) -- real images
            fake (tensor array) -- images generated by a generator

        Return the discriminator loss.
        We also call loss_D.backward() to calculate the gradients.
        """
        # TODO
        # Real
        pred_real = netD(real)
        print(pred_real)
        pred_fake = netD(fake)
        loss_D_real = self.criterion()
        # TODO: Loss definition
        pass

    def backward_D_A(self):
        pass

    def optimize_parameters(self):
        """Calculate loss functions, get gradients, update network weights."""
        pass
        # forward
        self.forward()      # 得到预测结果.
        self.set_requires_grad(self.netD, True) # 将待优化的 netD 设为 requires_grad
        self.optim
        # G_A and G_B
        self.set_requires_grad([self.netD_A, self.netD_B], False)  # Ds require no gradients when optimizing Gs
        self.optimizer_G.zero_grad()  # set G_A and G_B's gradients to zero
        self.backward_G()             # calculate gradients for G_A and G_B
        self.optimizer_G.step()       # update G_A and G_B's weights
        # D_A and D_B
        self.set_requires_grad([self.netD_A, self.netD_B], True)
        self.optimizer_D.zero_grad()   # set D_A and D_B's gradients to zero
        self.backward_D_A()      # calculate gradients for D_A
        self.backward_D_B()      # calculate graidents for D_B
        self.optimizer_D.step()  # update D_A and D_B's weights
